{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./X_train') as X_f, open('./Y_train')as y_f:\n",
    "    data = []\n",
    "    while True:\n",
    "        train_X = X_f.readline()\n",
    "        if(not train_X): #read until end of file\n",
    "            break\n",
    "        single_example = []\n",
    "        train_X = train_X.strip().split(',') #pre-process with '\\n' and ','\n",
    "        train_y = y_f.readline().strip().split(',')  #pre-process with '\\n' and ','\n",
    "        single_example.extend(train_X)\n",
    "        single_example.extend(train_y)\n",
    "        data.append(single_example)\n",
    "    data = np.array(data[1:]) #ignore row of attribute name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 107)\n"
     ]
    }
   ],
   "source": [
    "print(data.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardization\n",
    "train_X = np.array(data[:, :-1], dtype='float')\n",
    "train_y = np.array(data[:, -1:], dtype='float')\n",
    "\n",
    "std = train_X.std(axis=0)\n",
    "mean = train_X.mean(axis=0)\n",
    "train_X = (train_X - mean) / std\n",
    "        \n",
    "data = np.hstack((train_X, train_y))\n",
    "std.tofile('./stdandardize_std.model')\n",
    "mean.tofile('./stdandardize_mean.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    y = 1 / (1.0 + np.exp(-z))\n",
    "    return np.clip(y,1e-13,1-(1e-13)) #np.clip to avoid value overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cal_accurancy(data, w, b):\n",
    "    # count error with training set\n",
    "    train_X = np.array(data[:, :-1], dtype='float')\n",
    "    train_y = np.array(data[:, -1:], dtype='float').ravel()\n",
    "    predict = np.dot(train_X,w) + b\n",
    "    predict = sigmoid(predict)\n",
    "    \n",
    "    cross_entropy = -(np.dot(train_y, np.log(predict)) + np.dot((1-train_y), np.log(1-predict)))\n",
    "    classification = lambda i: 1 if i >= 0.5 else 0\n",
    "    predict = [classification(i) for i in predict]\n",
    "    \n",
    "    err_times = 0\n",
    "    for i in range(train_y.shape[0]):\n",
    "        if predict[i] == train_y[i]:\n",
    "            err_times += 1\n",
    "    print(err_times / train_y.shape[0], cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression_SGD(data, learning_rate = 1, epochs = 500):\n",
    "    batch_size = 25\n",
    "    # assume w and b with all ones will have nuch performance than zeros\n",
    "    w = np.zeros((106), dtype='float')\n",
    "    b = np.zeros((1), dtype='float')\n",
    "    #adagrad strategy\n",
    "    pre_w = np.zeros((106), dtype='float')\n",
    "    pre_b = np.zeros((1), dtype='float')\n",
    "    \n",
    "    z = np.ones((batch_size), dtype='float')\n",
    "    y = np.ones((batch_size), dtype='float')\n",
    "    \n",
    "    print('training start..')\n",
    "    for _ in range(epochs):\n",
    "        batch_iteration = 0\n",
    "        np.random.shuffle(data) # shuffle data to avoid SGD converge at local minimum\n",
    "        # an epoch\n",
    "        #in default, a epoch run 32561 / 25 = 1302... batch times\n",
    "        while batch_iteration * batch_size < (data.shape[0]): \n",
    "            #data extraction\n",
    "            batch_data = data[batch_iteration*batch_size : batch_iteration*batch_size+25]\n",
    "            train_X_batch = np.array(batch_data[:, :-1],dtype='float')\n",
    "            train_y_batch = np.array(batch_data[:, -1:],dtype='float').ravel() #change dimension to 1-D array\n",
    "            #print('train_X_batch.shape =',train_X_batch.shape,'\\n'+'w.shape =',w.shape,'\\n'+'b.shape =',b.shape)\n",
    "            #print('z.shape =',z.shape,'\\n'+'y.shape =',y.shape,'\\n'+'train_y_batch.shape =',train_y_batch.shape)\n",
    "\n",
    "            #predict\n",
    "            z = np.dot(train_X_batch,w) + b\n",
    "            y = sigmoid(z)\n",
    "\n",
    "            # update weights and bias\n",
    "            # use mean instead sum will imporve performance\n",
    "            w_gradient = np.mean(-1 * train_X_batch * (train_y_batch-y).reshape(batch_data.shape[0],1), axis = 0)\n",
    "            b_gradient = np.mean(-1 * (train_y_batch-y))\n",
    "            pre_w += w_gradient**2\n",
    "            pre_b += b_gradient**2\n",
    "            ada_w = np.sqrt(pre_w)\n",
    "            ada_b = np.sqrt(pre_b)\n",
    "\n",
    "            w = w - learning_rate * w_gradient / ada_w\n",
    "            b = b - learning_rate * b_gradient /ada_b\n",
    "            batch_iteration += 1\n",
    "            \n",
    "        cal_accurancy(data, w, b)\n",
    "    # save model\n",
    "    w.tofile('./w.model')\n",
    "    b.tofile('./b.model')\n",
    "    print('training finish..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start..\n",
      "0.8354473142716747 12528.9049103\n",
      "0.8422345751051872 11402.6980403\n",
      "0.8407297073185713 11325.7762638\n",
      "0.847394121802156 10850.9656495\n",
      "0.8476398145020116 10854.570958\n",
      "0.8436780197168392 10840.5837651\n",
      "0.8470255827523725 10717.0874564\n",
      "0.8469948711648905 10706.9596603\n",
      "0.8493903749884831 10656.2828349\n",
      "0.8487454316513621 10689.6876389\n",
      "0.8480390651392771 10742.5351824\n",
      "0.8491753938761094 10667.8714156\n",
      "0.8496360676883388 10689.8226173\n",
      "0.8484690273640244 10594.6079982\n",
      "0.8500660299130862 10553.9797409\n",
      "0.8498817603881945 10580.0617953\n",
      "0.8487147200638802 10550.2038184\n",
      "0.8491139707011456 10597.5531879\n",
      "0.8499124719756764 10583.269454\n",
      "0.8474248333896379 10612.8363882\n",
      "0.8493596634010012 10616.9495308\n",
      "0.8500967415005681 10543.8280611\n",
      "0.8498817603881945 10525.5677062\n",
      "0.8488989895887719 10512.5223811\n",
      "0.8509566659500629 10489.7602046\n",
      "0.8504038573753877 10487.1042794\n",
      "0.8496053561008569 10454.3984231\n",
      "0.8511409354749547 10569.711044\n",
      "0.8498203372132306 10508.8409255\n",
      "0.8499738951506404 10441.9993674\n",
      "0.8507416848376893 10474.2654031\n",
      "0.8500046067381223 10455.5725175\n",
      "0.8508031080126531 10481.1691139\n",
      "0.8490832591136636 10567.8848867\n",
      "0.8503424342004238 10477.2151085\n",
      "0.8501581646755321 10473.542674\n",
      "0.8514480513497743 10464.1317217\n",
      "0.8516016092871841 10436.8475767\n",
      "0.8504652805503516 10400.6888032\n",
      "0.8493903749884831 10509.9457187\n",
      "0.8514480513497743 10454.4208669\n",
      "0.8517244556371119 10423.7009552\n",
      "0.8507109732502073 10485.1071238\n",
      "0.8496053561008569 10474.4996538\n",
      "0.8510795122999908 10501.3617926\n",
      "0.8498203372132306 10444.6108439\n",
      "0.8505574153127975 10420.5099496\n",
      "0.8502502994379779 10431.79415\n",
      "0.8489297011762538 10465.6332293\n",
      "0.8508952427750991 10411.3273981\n",
      "0.8511409354749547 10427.1087106\n",
      "0.8504652805503516 10392.1118044\n",
      "0.8524922453241608 10395.793302\n",
      "0.8503424342004238 10414.1002494\n",
      "0.8515094745247381 10389.084939\n",
      "0.8496667792758208 10426.7301326\n",
      "0.8510795122999908 10409.5425304\n",
      "0.8507416848376893 10415.7718609\n",
      "0.8510488007125089 10420.9175162\n",
      "0.8504345689628697 10396.0181092\n",
      "0.8511409354749547 10409.3839316\n",
      "0.8511409354749547 10427.6185996\n",
      "0.8502810110254599 10402.6781779\n",
      "0.8501581646755321 10425.107362\n",
      "0.8498817603881945 10427.554543\n",
      "0.8504652805503516 10420.449915\n",
      "0.8510795122999908 10401.1747822\n",
      "0.8486840084763981 10522.997189\n",
      "0.8502502994379779 10434.8485575\n",
      "0.8512637818248825 10372.6397743\n",
      "0.8506802616627254 10417.029999\n",
      "0.8523079757992691 10400.525765\n",
      "0.8503731457879058 10397.0322449\n",
      "0.8521544178618593 10402.0044206\n",
      "0.8521851294493412 10391.261832\n",
      "0.850219587850496 10443.3798674\n",
      "0.8507416848376893 10427.3970364\n",
      "0.8515708976997021 10371.3095585\n",
      "0.8533521697736556 10391.3553234\n",
      "0.8504038573753877 10418.7372746\n",
      "0.8515094745247381 10372.2468079\n",
      "0.8513559165873285 10384.0728961\n",
      "0.8500046067381223 10409.3433536\n",
      "0.8516630324621479 10381.2052467\n",
      "0.8520315715119314 10358.8711633\n",
      "0.8507416848376893 10393.6383339\n",
      "0.8517551672245939 10380.2971058\n",
      "0.8518473019870397 10366.5334758\n",
      "0.8520929946868954 10395.4293212\n",
      "0.8500660299130862 10399.360212\n",
      "0.8514480513497743 10369.0533952\n",
      "0.8520929946868954 10348.6107461\n",
      "0.8508338196001352 10411.3464373\n",
      "0.8500967415005681 10389.2929023\n",
      "0.8522158410368232 10352.2122293\n",
      "0.8516016092871841 10382.9551821\n",
      "0.8514173397622923 10359.6957093\n",
      "0.8532293234237278 10382.7831538\n",
      "0.8526150916740887 10353.2296273\n",
      "0.8507109732502073 10366.432356\n",
      "0.8518780135745216 10354.2835642\n",
      "0.8522465526243052 10336.3342136\n",
      "0.8517858788120758 10342.3115307\n",
      "0.8506188384877614 10381.0554454\n",
      "0.8511102238874727 10373.3939784\n",
      "0.8504959921378336 10375.8177213\n",
      "0.8529529191363902 10357.4491263\n",
      "0.8526150916740887 10347.7652073\n",
      "0.8519701483369675 10372.6098528\n",
      "0.850925954362581 10346.9547615\n",
      "0.8522158410368232 10362.1892744\n",
      "0.8506495500752433 10409.5134706\n",
      "0.8520929946868954 10355.0673217\n",
      "0.851632320874666 10378.1743582\n",
      "0.8519701483369675 10338.2976581\n",
      "0.8511716470624366 10351.4877441\n",
      "0.8512637818248825 10388.9614821\n",
      "0.8516630324621479 10387.7857297\n",
      "0.8517551672245939 10348.9107784\n",
      "0.8523079757992691 10345.1971981\n",
      "0.8526765148490526 10397.936522\n",
      "0.8519394367494856 10354.3703584\n",
      "0.8533828813611376 10362.8925716\n",
      "0.851632320874666 10380.7309292\n",
      "0.8511102238874727 10371.3988797\n",
      "0.8514173397622923 10356.1858606\n",
      "0.8510180891250269 10376.2818865\n",
      "0.8493289518135192 10394.683657\n",
      "0.8517551672245939 10353.6435408\n",
      "0.8515708976997021 10355.2091736\n",
      "0.8527686496114985 10379.435949\n",
      "0.8526150916740887 10344.4548689\n",
      "0.8520622830994135 10344.9421863\n",
      "0.8508645311876171 10367.0474931\n",
      "0.8515708976997021 10368.6071876\n",
      "0.8513559165873285 10366.2204516\n",
      "0.8511102238874727 10358.731672\n",
      "0.8526765148490526 10344.5837194\n",
      "0.8525229569116428 10371.8677602\n",
      "0.8513252049998464 10344.6005343\n",
      "0.8518473019870397 10359.8039526\n",
      "0.8515401861122202 10375.4963015\n",
      "0.8526150916740887 10341.0459056\n",
      "0.8515401861122202 10365.6596089\n",
      "0.8513252049998464 10348.7311076\n",
      "0.8515708976997021 10341.6674149\n",
      "0.8515094745247381 10348.2985182\n",
      "0.8520315715119314 10340.1919376\n",
      "0.8525229569116428 10352.0031616\n",
      "0.8520929946868954 10329.2198871\n",
      "0.8526150916740887 10363.8879711\n",
      "0.8505574153127975 10372.0265893\n",
      "0.853045053898836 10348.0464695\n",
      "0.8529529191363902 10348.7021327\n",
      "0.8519394367494856 10356.5014293\n",
      "0.852338687386751 10342.3618087\n",
      "0.8534135929486195 10361.0605019\n",
      "0.8519701483369675 10329.9952879\n",
      "0.8519087251620037 10341.8543467\n",
      "0.8527379380240164 10330.1494778\n",
      "0.8514480513497743 10329.8680636\n",
      "0.8524615337366789 10335.9284775\n",
      "0.8521237062743773 10327.5992789\n",
      "0.8514787629372562 10351.9053244\n",
      "0.8522158410368232 10349.2236211\n",
      "0.8529836307238721 10338.5980959\n",
      "0.8506188384877614 10341.8358495\n",
      "0.8527686496114985 10350.2631514\n",
      "0.8519701483369675 10341.7992168\n",
      "0.8524922453241608 10338.6062945\n",
      "0.8522772642117871 10346.8121825\n",
      "0.8513866281748104 10345.6590447\n",
      "0.8521544178618593 10337.9720008\n",
      "0.8524922453241608 10362.3195456\n",
      "0.8522465526243052 10339.3246825\n",
      "0.8520315715119314 10368.9649907\n",
      "0.8526458032615706 10322.8974774\n",
      "0.8521544178618593 10335.0336625\n",
      "0.8525229569116428 10327.2786981\n",
      "0.8524001105617149 10337.2425559\n",
      "0.8520622830994135 10350.3376194\n",
      "0.8525536684991247 10343.5640008\n",
      "0.8533828813611376 10338.1261528\n",
      "0.8520315715119314 10327.8629028\n",
      "0.8514173397622923 10339.2646456\n",
      "0.8527993611989804 10337.7804845\n",
      "0.8518165903995577 10351.1979783\n",
      "0.8534750161235835 10332.2197902\n",
      "0.853075765486318 10328.1858608\n",
      "0.8530143423113541 10341.8670466\n",
      "0.8517551672245939 10334.0030063\n",
      "0.8519701483369675 10326.674829\n",
      "0.8534135929486195 10329.2997842\n",
      "0.8524922453241608 10329.4525379\n",
      "0.8528607843739443 10338.2073809\n",
      "0.8526150916740887 10333.5299173\n",
      "0.8516016092871841 10345.9459526\n",
      "0.8521851294493412 10338.3660926\n",
      "0.852338687386751 10333.9351951\n",
      "0.8513866281748104 10339.3125193\n",
      "0.8530143423113541 10332.2521524\n",
      "0.8519701483369675 10327.6539274\n",
      "0.852338687386751 10327.257534\n",
      "0.8519701483369675 10325.1021487\n",
      "0.8516016092871841 10345.6422717\n",
      "0.8525536684991247 10329.1172013\n",
      "0.8519701483369675 10331.1087005\n",
      "0.8516630324621479 10325.6033847\n",
      "0.8523079757992691 10331.7011785\n",
      "0.8522158410368232 10334.7253735\n",
      "0.8522158410368232 10317.8214756\n",
      "0.8535671508860293 10323.589021\n",
      "0.8518780135745216 10344.5090855\n",
      "0.8525229569116428 10343.8582383\n",
      "0.8514787629372562 10343.1822083\n",
      "0.8515094745247381 10322.105301\n",
      "0.8524308221491969 10332.6439609\n",
      "0.8531371886612819 10323.2846824\n",
      "0.8529222075489082 10328.3276142\n",
      "0.8527379380240164 10336.4681761\n",
      "0.852369398974233 10323.2324016\n",
      "0.8531371886612819 10330.4743418\n",
      "0.8526458032615706 10338.4236888\n",
      "0.8523079757992691 10335.2477844\n",
      "0.8506495500752433 10349.6682347\n",
      "0.8535057277110654 10310.0276135\n",
      "0.8520622830994135 10338.1789074\n",
      "0.8520929946868954 10333.4349936\n",
      "0.8523079757992691 10340.5449158\n",
      "0.8518473019870397 10336.2775662\n",
      "0.8528914959614262 10344.8566173\n",
      "0.8522465526243052 10331.3425358\n",
      "0.8514173397622923 10334.8250575\n",
      "0.8527072264365345 10335.2055288\n",
      "0.852338687386751 10342.4216315\n",
      "0.8521851294493412 10340.0282085\n",
      "0.8512637818248825 10346.4583192\n",
      "0.8513866281748104 10336.7718408\n",
      "0.8528607843739443 10328.5504477\n",
      "0.8526150916740887 10326.7548832\n",
      "0.8526458032615706 10318.2821345\n",
      "0.8524308221491969 10342.0691169\n",
      "0.8533521697736556 10311.57494\n",
      "0.8527993611989804 10329.7133811\n",
      "0.8526458032615706 10350.5173706\n",
      "0.853045053898836 10322.7931574\n",
      "0.8527072264365345 10323.4983474\n",
      "0.8527072264365345 10331.3337439\n",
      "0.8519394367494856 10325.5908329\n",
      "0.8521544178618593 10338.0425124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8525843800866066 10323.9653222\n",
      "0.8527072264365345 10324.8730604\n",
      "0.8527686496114985 10319.3781672\n",
      "0.8529529191363902 10323.5268773\n",
      "0.8528607843739443 10323.4677309\n",
      "0.8522158410368232 10332.1845074\n",
      "0.8525536684991247 10329.643066\n",
      "0.8512944934123645 10326.7055842\n",
      "0.8524922453241608 10313.775787\n",
      "0.8531064770737999 10330.5726776\n",
      "0.8524308221491969 10340.5861578\n",
      "0.8526765148490526 10335.1673197\n",
      "0.8531064770737999 10313.5338949\n",
      "0.8520315715119314 10329.1075213\n",
      "0.8521237062743773 10316.6507692\n",
      "0.8525536684991247 10326.3647685\n",
      "0.8521544178618593 10330.5334042\n",
      "0.853045053898836 10320.8051937\n",
      "0.8532907465986916 10317.0356984\n",
      "0.8527379380240164 10326.2331896\n",
      "0.8524922453241608 10335.4357255\n",
      "0.852338687386751 10327.0496417\n",
      "0.8531064770737999 10330.9139377\n",
      "0.8524001105617149 10332.1664186\n",
      "0.8524001105617149 10320.8874472\n",
      "0.8520315715119314 10316.0763902\n",
      "0.853045053898836 10319.4765514\n",
      "0.8528607843739443 10316.4745487\n",
      "0.8527379380240164 10329.3060267\n",
      "0.8533521697736556 10331.6865138\n",
      "0.8534443045361014 10315.3261426\n",
      "0.8527379380240164 10310.6899236\n",
      "0.8527379380240164 10316.61811\n",
      "0.8531986118362458 10327.7246058\n",
      "0.8516630324621479 10334.1557455\n",
      "0.8521851294493412 10320.6306123\n",
      "0.8522465526243052 10324.6494032\n",
      "0.8531371886612819 10333.263271\n",
      "0.8514480513497743 10329.7330148\n",
      "0.8528914959614262 10319.0109261\n",
      "0.8520008599244495 10334.4148166\n",
      "0.8532907465986916 10315.7429419\n",
      "0.8538742667608489 10323.4808246\n",
      "0.8524001105617149 10315.2889301\n",
      "0.8522158410368232 10314.1908159\n",
      "0.853045053898836 10336.5240865\n",
      "0.8519087251620037 10330.2190857\n",
      "0.8522772642117871 10313.3468422\n",
      "0.8523079757992691 10309.3248929\n",
      "0.8535671508860293 10312.2372667\n",
      "0.8531064770737999 10313.0935625\n",
      "0.8526765148490526 10314.9417562\n",
      "0.8524001105617149 10321.4223846\n",
      "0.8519087251620037 10339.4825488\n",
      "0.8522158410368232 10323.3964291\n",
      "0.8537207088234391 10306.6109965\n",
      "0.8525229569116428 10312.7306422\n",
      "0.8525843800866066 10319.4687793\n",
      "0.8527686496114985 10326.2576568\n",
      "0.852369398974233 10330.3071093\n",
      "0.8534750161235835 10313.7594586\n",
      "0.8526458032615706 10327.2079988\n",
      "0.8526765148490526 10310.517365\n",
      "0.8534750161235835 10315.5579782\n",
      "0.8508645311876171 10350.1546202\n",
      "0.8532293234237278 10307.4015966\n",
      "0.8523079757992691 10320.4187236\n",
      "0.8518165903995577 10316.4107381\n",
      "0.8522772642117871 10331.4127061\n",
      "0.8520622830994135 10316.1652009\n",
      "0.8524615337366789 10320.9936278\n",
      "0.8522772642117871 10326.5025886\n",
      "0.8517551672245939 10317.7365297\n",
      "0.8527072264365345 10320.9540159\n",
      "0.8528300727864623 10309.5018636\n",
      "0.8517244556371119 10324.2213123\n",
      "0.8531679002487639 10327.3825009\n",
      "0.8524308221491969 10313.8175545\n",
      "0.853045053898836 10324.374199\n",
      "0.8536592856484752 10310.1614057\n",
      "0.8522465526243052 10308.3119803\n",
      "0.8535364392985474 10310.8401213\n",
      "0.8531679002487639 10314.8002496\n",
      "0.8520315715119314 10313.2655223\n",
      "0.852369398974233 10310.558865\n",
      "0.8524922453241608 10323.7146946\n",
      "0.8520315715119314 10317.8163548\n",
      "0.8531679002487639 10320.8876827\n",
      "0.8522158410368232 10316.7113655\n",
      "0.853075765486318 10323.0534163\n",
      "0.8517551672245939 10325.5567777\n",
      "0.8531679002487639 10314.5312216\n",
      "0.8527686496114985 10310.7698565\n",
      "0.8524308221491969 10307.3505102\n",
      "0.8520622830994135 10324.6531455\n",
      "0.8528914959614262 10303.4382196\n",
      "0.8524922453241608 10308.8161432\n",
      "0.8530143423113541 10305.1151546\n",
      "0.8517858788120758 10306.2802205\n",
      "0.8526458032615706 10308.6550323\n",
      "0.8521237062743773 10315.6963057\n",
      "0.8522158410368232 10311.4745481\n",
      "0.8531679002487639 10319.2586468\n",
      "0.8520315715119314 10315.1625964\n",
      "0.8524922453241608 10320.0297949\n",
      "0.8515094745247381 10317.8545422\n",
      "0.852338687386751 10300.3667208\n",
      "0.8524922453241608 10319.3559247\n",
      "0.8512944934123645 10312.2720146\n",
      "0.8531986118362458 10309.0710353\n",
      "0.8527686496114985 10313.262019\n",
      "0.8533521697736556 10319.3494818\n",
      "0.8528300727864623 10300.1109923\n",
      "0.8527686496114985 10301.7418855\n",
      "0.8534135929486195 10315.1634492\n",
      "0.8531986118362458 10308.241201\n",
      "0.8519701483369675 10317.7552063\n",
      "0.8531679002487639 10311.2021963\n",
      "0.8519701483369675 10328.6836825\n",
      "0.8518165903995577 10318.0920216\n",
      "0.8521544178618593 10305.0328112\n",
      "0.8529529191363902 10306.009865\n",
      "0.8519394367494856 10314.6117818\n",
      "0.8522158410368232 10318.4471727\n",
      "0.8521851294493412 10320.5553581\n",
      "0.8525229569116428 10313.1082729\n",
      "0.8516016092871841 10322.7157591\n",
      "0.8527993611989804 10312.5031421\n",
      "0.8519701483369675 10322.0241132\n",
      "0.8523079757992691 10315.0095027\n",
      "0.8525843800866066 10308.7878659\n",
      "0.8521544178618593 10312.5753308\n",
      "0.8532293234237278 10315.3842486\n",
      "0.8527379380240164 10307.665787\n",
      "0.8532907465986916 10309.6691222\n",
      "0.8534443045361014 10306.3832658\n",
      "0.8529222075489082 10305.9225929\n",
      "0.8512944934123645 10320.7574165\n",
      "0.8521544178618593 10306.6151692\n",
      "0.8524922453241608 10313.5675886\n",
      "0.8524922453241608 10307.2616005\n",
      "0.8527379380240164 10307.4807236\n",
      "0.8527686496114985 10310.679129\n",
      "0.8520929946868954 10319.9427846\n",
      "0.853045053898836 10310.6016025\n",
      "0.8521851294493412 10312.2880294\n",
      "0.8532293234237278 10315.4524863\n",
      "0.8530143423113541 10307.4977152\n",
      "0.8533828813611376 10306.4299312\n",
      "0.8527686496114985 10315.2982816\n",
      "0.8526458032615706 10308.0823733\n",
      "0.8526765148490526 10314.6506382\n",
      "0.8529529191363902 10311.9207884\n",
      "0.8526765148490526 10310.004323\n",
      "0.8532907465986916 10311.8127589\n",
      "0.8531064770737999 10320.0370342\n",
      "0.8526150916740887 10309.3415443\n",
      "0.8524308221491969 10306.4051972\n",
      "0.8524922453241608 10315.058308\n",
      "0.8517551672245939 10300.7192006\n",
      "0.8529529191363902 10315.59904\n",
      "0.8539664015232947 10300.266531\n",
      "0.8527379380240164 10311.6595748\n",
      "0.8535364392985474 10308.161235\n",
      "0.8524615337366789 10318.066133\n",
      "0.8521544178618593 10319.120228\n",
      "0.852369398974233 10303.2702397\n",
      "0.8522772642117871 10306.3762347\n",
      "0.8527686496114985 10299.6118293\n",
      "0.8508338196001352 10320.0063205\n",
      "0.8533521697736556 10314.2341513\n",
      "0.8527072264365345 10302.5138198\n",
      "0.8524922453241608 10312.434502\n",
      "0.8524615337366789 10317.1539406\n",
      "0.8524922453241608 10310.2319929\n",
      "0.8524922453241608 10333.3334305\n",
      "0.8526150916740887 10310.919467\n",
      "0.8532600350112097 10313.3207686\n",
      "0.8540892478732226 10305.8510186\n",
      "0.8520315715119314 10316.8539363\n",
      "0.8526150916740887 10304.1573083\n",
      "0.8528300727864623 10307.0956793\n",
      "0.852338687386751 10306.4346344\n",
      "0.8526150916740887 10312.4708269\n",
      "0.853843555173367 10304.2444385\n",
      "0.8527379380240164 10303.1074471\n",
      "0.8529529191363902 10306.5001329\n",
      "0.8535671508860293 10298.3731785\n",
      "0.8520622830994135 10305.4663802\n",
      "0.8527686496114985 10304.8974612\n",
      "0.8519394367494856 10309.8895634\n",
      "0.8527072264365345 10307.2985752\n",
      "0.8519087251620037 10308.7018217\n",
      "0.8532907465986916 10305.8037584\n",
      "0.8529529191363902 10298.979004\n",
      "0.8527379380240164 10310.3387335\n",
      "0.8531371886612819 10305.2766731\n",
      "0.8532907465986916 10307.3621732\n",
      "0.8527072264365345 10312.5422391\n",
      "0.852369398974233 10308.8103109\n",
      "0.8523079757992691 10321.4165486\n",
      "0.8526458032615706 10305.5913535\n",
      "0.8535057277110654 10305.5860831\n",
      "0.8527686496114985 10303.6386802\n",
      "0.8535364392985474 10310.3816623\n",
      "0.8526765148490526 10302.8463654\n",
      "0.853075765486318 10307.1549132\n",
      "0.8536899972359572 10305.513379\n",
      "0.8522465526243052 10318.816912\n",
      "0.8528914959614262 10303.5691068\n",
      "0.8522772642117871 10302.7725414\n",
      "0.8521851294493412 10313.7867242\n",
      "0.853045053898836 10301.3853049\n",
      "0.8529529191363902 10313.2255654\n",
      "0.8529222075489082 10316.8373439\n",
      "0.8529529191363902 10301.5135616\n",
      "0.8522465526243052 10311.0382331\n",
      "0.8520929946868954 10307.2147274\n",
      "0.8521851294493412 10305.1340293\n",
      "0.8525536684991247 10313.8543107\n",
      "0.8534443045361014 10303.4128293\n",
      "0.8526765148490526 10299.6295578\n",
      "0.8523079757992691 10304.5485898\n",
      "0.8534135929486195 10299.5753638\n",
      "0.8535364392985474 10299.9105063\n",
      "0.8534135929486195 10304.1113729\n",
      "0.8525536684991247 10312.143426\n",
      "0.8525536684991247 10302.7576335\n",
      "0.8525843800866066 10305.0531714\n",
      "0.8526458032615706 10300.2154495\n",
      "0.852338687386751 10302.8334689\n",
      "0.8521237062743773 10299.4989745\n",
      "0.8518473019870397 10314.1269152\n",
      "0.8528300727864623 10309.6871675\n",
      "0.8531986118362458 10300.7017063\n",
      "0.8528300727864623 10302.1578161\n",
      "0.8531679002487639 10307.6977453\n",
      "0.8527686496114985 10302.6680766\n",
      "0.853075765486318 10303.3916249\n",
      "0.8524615337366789 10306.4144331\n",
      "0.8524001105617149 10304.0573536\n",
      "0.8517551672245939 10302.7161198\n",
      "0.8521851294493412 10306.3236803\n",
      "0.8532907465986916 10306.5493492\n",
      "0.8528607843739443 10303.1824169\n",
      "0.8523079757992691 10302.7066876\n",
      "0.8528914959614262 10308.063839\n",
      "0.8536592856484752 10303.543692\n",
      "0.853075765486318 10303.9155376\n",
      "0.8526458032615706 10310.1361928\n",
      "training finish..\n"
     ]
    }
   ],
   "source": [
    "logistic_regression_SGD(data, learning_rate = 1, epochs = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testing(filename = './X_test'):\n",
    "    print('Starting testing..')\n",
    "    #read testing dataset\n",
    "    with open(filename) as f:\n",
    "        test_data = []\n",
    "        test_num = 0\n",
    "        while True:\n",
    "            test_X = f.readline()\n",
    "            if(not test_X): #read until end of file\n",
    "                break\n",
    "            test_num += 1\n",
    "            single_example = []\n",
    "            test_X = test_X.strip().split(',') #pre-process with '\\n' and ','\n",
    "            single_example.extend(test_X)\n",
    "            test_data.append(single_example)\n",
    "        test_data = np.array(test_data[1:]).astype('float') #ignore row of attribute name\n",
    "        \n",
    "        #loading model\n",
    "        w = np.fromfile('./w.model')\n",
    "        b = np.fromfile('./b.model')\n",
    "        std = np.fromfile('./stdandardize_std.model')\n",
    "        mean = np.fromfile('./stdandardize_mean.model')\n",
    "        test_data = (test_data - mean) / std #Standardization\n",
    "\n",
    "        #predict\n",
    "        z = np.dot(test_data,w) + b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "        #write to file\n",
    "        write_to_file(y)\n",
    "    print('Testing finishing..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_to_file(test_y,filename = './predictions.csv'):\n",
    "    with open(filename, 'w') as f:\n",
    "        classification = lambda i: 1 if i >= 0.5 else 0\n",
    "        predict = [classification(i) for i in test_y]\n",
    "        f.write('id,label\\n')\n",
    "        for i in range(len(predict)):\n",
    "            f.write(str(i) + ',' + str(predict[i]) + '\\n')\n",
    "            #print(test_y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
